# -*- coding: utf-8 -*-
"""traffic_detectV1.1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c-lurp77dpgOSH0X5jG60cfliuRFUxuI
"""

!pip install tensorflow
!pip install keras
!pip install opencv-python-headless

import numpy as np
import matplotlib.pyplot as plt
from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from tensorflow.keras.optimizers import Adam
from keras.utils import to_categorical
from keras.layers import Dropout, Flatten, Conv2D, MaxPooling2D
import cv2
from sklearn.model_selection import train_test_split
import pickle
import os
import pandas as pd
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import zipfile
import io

# Define paths and hyperparameters
zip_path = "/content/drive/MyDrive/Dataset.zip"  # Path to zip file
labelFile = 'labels.csv'
batch_size_val = 32
epochs_val = 20
imageDimensions = (32, 32, 3)
testRatio = 0.2
validationRatio = 0.2

count = 0
images = []
classNo = []

# Process zip file directly
print("Processing zip file directly...")
try:
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        # Get all file names in the zip
        file_list = zip_ref.namelist()

        # Find class directories (assuming structure like: Dataset/0/, Dataset/1/, etc.)
        class_dirs = set()
        image_files = []

        for file_path in file_list:
            if file_path.endswith('/'):  # It's a directory
                continue

            # Extract directory structure
            parts = file_path.split('/')
            if len(parts) >= 2:
                # Assume structure: [root_folder]/[class_folder]/[image_file]
                class_folder = parts[-2]  # Second to last part is class folder

                # Check if this looks like a class folder (numeric)
                try:
                    class_num = int(class_folder)
                    class_dirs.add(class_folder)
                    image_files.append((file_path, class_num))
                except ValueError:
                    continue

        # Sort class directories
        sorted_classes = sorted(class_dirs, key=lambda x: int(x))
        noOfClasses = len(sorted_classes)

        print(f"Total Classes Detected: {noOfClasses}")
        print(f"Classes: {sorted_classes}")

        if noOfClasses == 0:
            print("Error: No valid class directories found in zip file.")
            exit()

        print("Loading images from zip file...")

        # Process each image file
        for file_path, class_num in image_files:
            try:
                # Read image data from zip
                image_data = zip_ref.read(file_path)

                # Convert bytes to numpy array
                nparr = np.frombuffer(image_data, np.uint8)

                # Decode image
                img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

                if img is not None:
                    images.append(img)
                    classNo.append(class_num)
                else:
                    print(f"Warning: Could not decode image {file_path}")

            except Exception as e:
                print(f"Warning: Error processing {file_path}: {e}")

        print(f"Successfully loaded {len(images)} images")

        # Try to read labels.csv if it exists in the zip
        try:
            labels_data = zip_ref.read(labelFile)
            # Convert bytes to string and then to pandas DataFrame
            labels_str = labels_data.decode('utf-8')
            data = pd.read_csv(io.StringIO(labels_str))
            print("Label data shape:", data.shape, type(data))
        except KeyError:
            print(f"Warning: Label file '{labelFile}' not found in zip.")
        except Exception as e:
            print(f"Warning: Error reading label file: {e}")

except FileNotFoundError:
    print(f"Error: Zip file not found at {zip_path}")
    print("Please check the file path and ensure the zip file exists.")
    exit()
except zipfile.BadZipFile:
    print(f"Error: {zip_path} is not a valid zip file.")
    exit()

# Convert to numpy arrays
images = np.array(images)
classNo = np.array(classNo)

if len(images) == 0:
    print("Error: No images loaded. Check dataset structure in zip file.")
    exit()

print(f"Total images loaded: {len(images)}")

# Train/test/validation split
X_train, X_test, y_train, y_test = train_test_split(images, classNo, test_size=testRatio, random_state=42)
X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validationRatio, random_state=42)

print("Data Shapes")
print("Train:", X_train.shape, y_train.shape)
print("Validation:", X_validation.shape, y_validation.shape)
print("Test:", X_test.shape, y_test.shape)

def grayscale(img):
    if img is None:
        return None
    return cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

def equalize(img):
    if img is None:
        return None
    if len(img.shape) == 2 and img.dtype == np.uint8:
        return cv2.equalizeHist(img)
    else:
        print(f"Warning: EqualizeHist requires 8-bit single-channel image, got shape {img.shape} and dtype {img.dtype}.")
        return img

def preprocessing(img):
    if img is None:
        return None

    # Resize image to target dimensions
    img = cv2.resize(img, (imageDimensions[0], imageDimensions[1]))
    img = grayscale(img)
    if img is None:
        return None
    img = equalize(img)
    if img is None:
        return None
    img = img / 255.0
    return img

# Preprocess images
print("Preprocessing images...")
X_train_processed = []
X_validation_processed = []
X_test_processed = []
y_train_filtered = []
y_validation_filtered = []
y_test_filtered = []

# Process training data
for i, img in enumerate(X_train):
    processed = preprocessing(img)
    if processed is not None:
        X_train_processed.append(processed)
        y_train_filtered.append(y_train[i])

# Process validation data
for i, img in enumerate(X_validation):
    processed = preprocessing(img)
    if processed is not None:
        X_validation_processed.append(processed)
        y_validation_filtered.append(y_validation[i])

# Process test data
for i, img in enumerate(X_test):
    processed = preprocessing(img)
    if processed is not None:
        X_test_processed.append(processed)
        y_test_filtered.append(y_test[i])

# Convert to numpy arrays
X_train = np.array(X_train_processed)
X_validation = np.array(X_validation_processed)
X_test = np.array(X_test_processed)
y_train = np.array(y_train_filtered)
y_validation = np.array(y_validation_filtered)
y_test = np.array(y_test_filtered)

print("Shapes after preprocessing:")
print("Train:", X_train.shape, y_train.shape)
print("Validation:", X_validation.shape, y_validation.shape)
print("Test:", X_test.shape, y_test.shape)

# Reshape for CNN input (add channel dimension)
if X_train.ndim == 3:
    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)
if X_validation.ndim == 3:
    X_validation = X_validation.reshape(X_validation.shape[0], X_validation.shape[1], X_validation.shape[2], 1)
if X_test.ndim == 3:
    X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)

print("Shapes after reshaping:")
print("Train:", X_train.shape)
print("Validation:", X_validation.shape)
print("Test:", X_test.shape)

# Data Augmentation
dataGen = ImageDataGenerator(width_shift_range=0.1,
                            height_shift_range=0.1,
                            zoom_range=0.2,
                            shear_range=0.1,
                            rotation_range=10)
dataGen.fit(X_train)

# Convert labels to categorical format
y_train = to_categorical(y_train, noOfClasses)
y_validation = to_categorical(y_validation, noOfClasses)
y_test = to_categorical(y_test, noOfClasses)

def myModel():
    model = Sequential()
    # Corrected input shape to match preprocessed images (grayscale)
    model.add(Conv2D(60, (5, 5), input_shape=(imageDimensions[0], imageDimensions[1], 1), activation='relu'))
    model.add(Conv2D(60, (5, 5), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))

    model.add(Conv2D(30, (3, 3), activation='relu'))
    model.add(Conv2D(30, (3, 3), activation='relu'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Dropout(0.5))

    model.add(Flatten())
    model.add(Dense(500, activation='relu'))
    model.add(Dropout(0.5))
    model.add(Dense(noOfClasses, activation='softmax'))

    model.compile(Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])
    return model

# Create and train the model
if X_train.shape[0] > 0:
    model = myModel()
    print(model.summary())

    # Calculate steps per epoch
    steps_per_epoch_val = len(X_train) // batch_size_val
    if steps_per_epoch_val == 0:
        steps_per_epoch_val = 1

    print("Starting training...")
    history = model.fit(
        dataGen.flow(X_train, y_train, batch_size=batch_size_val),
        steps_per_epoch=steps_per_epoch_val,
        epochs=epochs_val,
        validation_data=(X_validation, y_validation),
        shuffle=True
    )

    # Plot training history
    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.legend(['training', 'validation'])
    plt.title('Loss')
    plt.xlabel('Epoch')

    plt.subplot(1, 2, 2)
    plt.plot(history.history['accuracy'])
    plt.plot(history.history['val_accuracy'])
    plt.legend(['training', 'validation'])
    plt.title('Accuracy')
    plt.xlabel('Epoch')

    plt.tight_layout()
    plt.show()

    # Evaluate the model
    score = model.evaluate(X_test, y_test, verbose=0)
    print('Test Score:', score[0])
    print('Test Accuracy:', score[1])

    # Save the model
    model.save("model.h5")
    print("Model saved as model.h5")

else:
    print("Skipping model training due to empty training dataset.")

# --- 9. Test with Uploaded Images ---
from google.colab import files
import ipywidgets as widgets
from IPython.display import display, Image as IPImage # Import Image for displaying


# Preprocessing function for a single image (should match training preprocessing)
def preprocess_single_image(img_array, target_height=IMG_HEIGHT, target_width=IMG_WIDTH):
    # Assuming img_array is a NumPy array from cv2.imread (BGR)
    img_resized = cv2.resize(img_array, (target_width, target_height))
    img_gray = cv2.cvtColor(img_resized, cv2.COLOR_BGR2GRAY)
    img_equalized = cv2.equalizeHist(img_gray)
    img_normalized = img_equalized / 255.0
    # Reshape for model input: (1, height, width, channels)
    img_reshaped = img_normalized.reshape(1, target_height, target_width, CHANNELS)
    return img_reshaped

# Function to get class name from class ID
# Ensure 'class_names' list is populated correctly from your labels.csv
# or use fallback if class_names is empty
def get_class_name(class_id_val):
    if class_names and 0 <= class_id_val < len(class_names):
        return class_names[class_id_val]
    else:
        return f"Class ID: {class_id_val}" # Fallback if names not available

# Create a file upload button
uploader = widgets.FileUpload(
    accept='image/*',  # Accept all image types
    multiple=True      # Allow multiple files to be uploaded
)

print("Upload your traffic sign images:")
display(uploader)

# Function to handle uploaded files and make predictions
def on_upload_change(change):
    if not change['new']: # No new files
        return

    for name, file_info in uploader.value.items():
        print(f"\nProcessing: {name}")
        # Display the uploaded image
        display(IPImage(data=file_info['content']))

        # Convert uploaded file content to an OpenCV image
        nparr = np.frombuffer(file_info['content'], np.uint8)
        img_bgr = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if img_bgr is None:
            print("Could not decode image. Please upload a valid image file.")
            continue

        # Preprocess the image
        processed_img = preprocess_single_image(img_bgr)

        # Make a prediction
        predictions_probs = model.predict(processed_img)
        predicted_class_id = np.argmax(predictions_probs, axis=1)[0]
        confidence = np.max(predictions_probs) * 100

        predicted_class_name = get_class_name(predicted_class_id)

        print(f"Predicted Sign: {predicted_class_name}")
        print(f"Confidence: {confidence:.2f}%")
        print(f"Raw Probabilities: {predictions_probs[0]}") # Optional: show all probabilities

    # Clear the uploader for next use
    uploader.value.clear()
    uploader._counter = 0 # Reset counter to allow re-uploading same named file if needed


uploader.observe(on_upload_change, names='value')

from google.colab import drive
drive.mount('/content/drive')